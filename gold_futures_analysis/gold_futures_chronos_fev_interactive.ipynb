{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold Futures Forecasting with Chronos-Bolt-Base Model\n",
    "\n",
    "## Comprehensive Performance Evaluation with Interactive Visualizations\n",
    "\n",
    "### Objectives\n",
    "1. Test Chronos-Bolt-Base model performance on gold futures forecasting\n",
    "2. Use rolling 3-month windows for next-day predictions\n",
    "3. Evaluate on 2020-2021 historical data\n",
    "4. Compare against baseline models using FEV framework\n",
    "5. Provide interactive visualizations with zoom capabilities\n",
    "\n",
    "### Methodology\n",
    "- **Data**: GCUSD (Gold Futures) daily OHLCV data from 2020-2021\n",
    "- **Model**: Chronos-Bolt-Base (`amazon/chronos-bolt-base`)\n",
    "- **Evaluation**: Rolling window approach with 63 trading days (3 months) context\n",
    "- **Benchmarking**: FEV framework with standardized metrics\n",
    "- **Visualization**: Interactive plots with Plotly and Bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing required packages...\")\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_package(package_name, alternative_name=None):\n",
    "    \"\"\"Install a package with fallback options\"\"\"\n",
    "    try:\n",
    "        # Try pip install first\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--quiet\"], \n",
    "                      check=True, capture_output=True)\n",
    "        print(f\"✅ {package_name} installed via pip\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        # If pip fails, try with --break-system-packages (not recommended but sometimes necessary)\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--break-system-packages\", \"--quiet\"], \n",
    "                          check=True, capture_output=True)\n",
    "            print(f\"✅ {package_name} installed via pip (system packages)\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError:\n",
    "            # If still fails, try system package manager\n",
    "            if alternative_name:\n",
    "                try:\n",
    "                    subprocess.run([\"apt\", \"install\", \"-y\", alternative_name], \n",
    "                                  check=True, capture_output=True)\n",
    "                    print(f\"✅ {alternative_name} installed via apt\")\n",
    "                    return True\n",
    "                except subprocess.CalledProcessError:\n",
    "                    pass\n",
    "            print(f\"❌ Failed to install {package_name}\")\n",
    "            return False\n",
    "\n",
    "# Install packages in order of importance\n",
    "packages = [\n",
    "    (\"pandas\", \"python3-pandas\"),\n",
    "    (\"numpy\", \"python3-numpy\"),\n",
    "    (\"matplotlib\", \"python3-matplotlib\"),\n",
    "    (\"seaborn\", \"python3-seaborn\"),\n",
    "    (\"scipy\", \"python3-scipy\"),\n",
    "    (\"scikit-learn\", \"python3-sklearn\"),\n",
    "    (\"torch\", None),  # PyTorch for Chronos\n",
    "    (\"chronos-forecasting\", None),\n",
    "    (\"fev\", None),  # FEV - Forecast Evaluation Framework\n",
    "    (\"datasets\", None),  # Hugging Face datasets (required for FEV)\n",
    "    (\"plotly\", \"python3-plotly\"),\n",
    "    (\"bokeh\", \"python3-bokeh\"),\n",
    "    (\"ipywidgets\", \"python3-ipywidgets\")\n",
    "]\n",
    "\n",
    "print(\"Installing core packages...\")\n",
    "for package, alt_name in packages:\n",
    "    install_package(package, alt_name)\n",
    "\n",
    "print(\"\\nPackage installation completed!\")\n",
    "print(\"Verifying FEV installation...\")\n",
    "\n",
    "# Verify FEV installation specifically\n",
    "try:\n",
    "    import fev\n",
    "    print(\"✅ FEV successfully imported!\")\n",
    "    print(f\"FEV version info: {fev.__version__ if hasattr(fev, '__version__') else 'Version not available'}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ FEV import failed: {e}\")\n",
    "    print(\"Attempting alternative FEV installation...\")\n",
    "    \n",
    "    # Try installing FEV with different methods\n",
    "    try:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"fev\", \"--break-system-packages\", \"--quiet\"], \n",
    "                      check=True, capture_output=True)\n",
    "        import fev\n",
    "        print(\"✅ FEV installed and imported successfully!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ FEV installation failed: {e2}\")\n",
    "        print(\"Will use alternative evaluation framework...\")\n",
    "\n",
    "print(\"\\nAll installations completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core plotting libraries\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(\"✅ Matplotlib and seaborn imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing matplotlib/seaborn: {e}\")\n",
    "    print(\"Installing matplotlib and seaborn...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'matplotlib', 'seaborn', '--break-system-packages', '--quiet'])\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(\"✅ Matplotlib and seaborn installed and imported\")\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Chronos imports\n",
    "try:\n",
    "    import torch\n",
    "    from chronos import BaseChronosPipeline\n",
    "    print(\"✅ Chronos imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing Chronos: {e}\")\n",
    "    print(\"Installing chronos-forecasting...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'chronos-forecasting', '--break-system-packages', '--quiet'])\n",
    "    import torch\n",
    "    from chronos import BaseChronosPipeline\n",
    "    print(\"✅ Chronos installed and imported\")\n",
    "\n",
    "# FEV imports (Forecast Evaluation Framework)\n",
    "fev_available = False\n",
    "try:\n",
    "    import fev\n",
    "    from datasets import Dataset\n",
    "    fev_available = True\n",
    "    print(\"✅ FEV imports successful\")\n",
    "    print(f\"FEV version: {fev.__version__ if hasattr(fev, '__version__') else 'Version info not available'}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ FEV not available: {e}\")\n",
    "    print(\"Installing FEV and datasets...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    try:\n",
    "        # Install FEV and its dependencies\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', 'fev', '--break-system-packages', '--quiet'])\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', 'datasets', '--break-system-packages', '--quiet'])\n",
    "        \n",
    "        # Try importing again\n",
    "        import fev\n",
    "        from datasets import Dataset\n",
    "        fev_available = True\n",
    "        print(\"✅ FEV installed and imported successfully\")\n",
    "    except Exception as e2:\n",
    "        print(f\"⚠️ FEV installation failed: {e2}\")\n",
    "        print(\"Will use alternative evaluation framework (datasets only)\")\n",
    "        try:\n",
    "            from datasets import Dataset\n",
    "            print(\"✅ Datasets imported as FEV alternative\")\n",
    "        except ImportError:\n",
    "            print(\"❌ Datasets also not available, using custom evaluation\")\n",
    "\n",
    "# Interactive visualization imports with better dependency handling\n",
    "plotly_available = False\n",
    "try:\n",
    "    # Install required dependencies first\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'nbformat>=4.2.0', 'ipython', '--break-system-packages', '--quiet'])\n",
    "    \n",
    "    # Now try importing plotly\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.io as pio\n",
    "    \n",
    "    # Set renderer for notebook environment\n",
    "    pio.renderers.default = \"plotly_mimetype+notebook\"\n",
    "    plotly_available = True\n",
    "    print(\"✅ Plotly imports successful with notebook renderer\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing Plotly: {e}\")\n",
    "    print(\"Installing plotly...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'plotly', '--break-system-packages', '--quiet'])\n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "        import plotly.express as px\n",
    "        from plotly.subplots import make_subplots\n",
    "        import plotly.io as pio\n",
    "        pio.renderers.default = \"plotly_mimetype+notebook\"\n",
    "        plotly_available = True\n",
    "        print(\"✅ Plotly installed and imported\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️ Plotly not available, will use matplotlib fallbacks\")\n",
    "        plotly_available = False\n",
    "\n",
    "# Bokeh imports (optional)\n",
    "try:\n",
    "    from bokeh.plotting import figure, show, output_notebook\n",
    "    from bokeh.layouts import column, row\n",
    "    from bokeh.models import HoverTool, DatetimeTickFormatter, Select, CheckboxGroup\n",
    "    from bokeh.io import push_notebook\n",
    "    output_notebook()\n",
    "    print(\"✅ Bokeh imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Bokeh not available: {e}\")\n",
    "    print(\"Bokeh visualizations will be skipped\")\n",
    "\n",
    "# Jupyter widgets (optional)\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML\n",
    "    print(\"✅ Jupyter widgets imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Jupyter widgets not available: {e}\")\n",
    "    print(\"Interactive widgets will be simplified\")\n",
    "\n",
    "# Statistical analysis\n",
    "try:\n",
    "    from scipy import stats\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    print(\"✅ Statistical analysis imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing scipy/sklearn: {e}\")\n",
    "    print(\"Installing scipy and scikit-learn...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scipy', 'scikit-learn', '--break-system-packages', '--quiet'])\n",
    "    from scipy import stats\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    print(\"✅ Statistical analysis packages installed and imported\")\n",
    "\n",
    "print(f\"\\n🎉 All libraries imported successfully!\")\n",
    "print(f\"FEV Framework Available: {'✅ Yes' if fev_available else '❌ No (using alternatives)'}\")\n",
    "print(f\"Plotly Available: {'✅ Yes' if plotly_available else '❌ No (using matplotlib)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gold futures data\n",
    "try:\n",
    "    df = pd.read_csv('GCUSD_MAX_FROM_PERPLEXITY.csv')\n",
    "    print(\"✅ Data loaded successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: GCUSD_MAX_FROM_PERPLEXITY.csv not found\")\n",
    "    print(\"Please ensure the data file is in the current directory.\")\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    import numpy as np\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    # Generate sample gold futures data\n",
    "    np.random.seed(42)\n",
    "    start_date = datetime(2020, 1, 1)\n",
    "    end_date = datetime(2021, 12, 31)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # Filter for weekdays only (trading days)\n",
    "    trading_days = [d for d in date_range if d.weekday() < 5]\n",
    "    \n",
    "    # Generate realistic gold price data with trend and noise\n",
    "    n_days = len(trading_days)\n",
    "    base_price = 1800  # Starting gold price\n",
    "    trend = np.linspace(0, 200, n_days)  # Upward trend\n",
    "    noise = np.random.normal(0, 20, n_days)  # Daily volatility\n",
    "    \n",
    "    # Generate OHLC data\n",
    "    close_prices = base_price + trend + noise\n",
    "    open_prices = close_prices + np.random.normal(0, 5, n_days)\n",
    "    high_prices = np.maximum(open_prices, close_prices) + np.abs(np.random.normal(0, 10, n_days))\n",
    "    low_prices = np.minimum(open_prices, close_prices) - np.abs(np.random.normal(0, 10, n_days))\n",
    "    volume = np.random.lognormal(10, 0.5, n_days).astype(int)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Date': trading_days,\n",
    "        'Open': open_prices,\n",
    "        'High': high_prices,\n",
    "        'Low': low_prices,\n",
    "        'Close': close_prices,\n",
    "        'Volume': volume\n",
    "    })\n",
    "    \n",
    "    print(f\"✅ Sample data created with {len(df)} trading days\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"First few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess gold futures data for time series analysis\n",
    "    \"\"\"\n",
    "    # Create a copy\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Convert Date column to datetime\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    \n",
    "    # Sort by date (ascending)\n",
    "    data = data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Filter for 2020-2021 data\n",
    "    mask = (data['Date'] >= '2020-01-01') & (data['Date'] <= '2021-12-31')\n",
    "    data = data[mask].reset_index(drop=True)\n",
    "    \n",
    "    # Handle missing values if any (using forward fill)\n",
    "    data = data.fillna(method='ffill')\n",
    "    \n",
    "    # Create target variable (next day's close price)\n",
    "    data['Target'] = data['Close'].shift(-1)\n",
    "    \n",
    "    # Remove last row (no target available)\n",
    "    data = data[:-1].reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Preprocess the data\n",
    "try:\n",
    "    data = preprocess_data(df)\n",
    "    print(\"✅ Data preprocessing completed\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during preprocessing: {e}\")\n",
    "    print(\"Attempting alternative preprocessing...\")\n",
    "    \n",
    "    # Alternative preprocessing without deprecated method\n",
    "    data = df.copy()\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data = data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Filter for 2020-2021 data\n",
    "    mask = (data['Date'] >= '2020-01-01') & (data['Date'] <= '2021-12-31')\n",
    "    data = data[mask].reset_index(drop=True)\n",
    "    \n",
    "    # Handle missing values using forward fill (newer method)\n",
    "    data = data.ffill()\n",
    "    \n",
    "    # Create target variable (next day's close price)\n",
    "    data['Target'] = data['Close'].shift(-1)\n",
    "    \n",
    "    # Remove last row (no target available)\n",
    "    data = data[:-1].reset_index(drop=True)\n",
    "    \n",
    "    print(\"✅ Alternative preprocessing completed\")\n",
    "\n",
    "print(f\"Filtered dataset shape: {data.shape}\")\n",
    "print(f\"Date range: {data['Date'].min()} to {data['Date'].max()}\")\n",
    "print(f\"Number of trading days: {len(data)}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(data[['Open', 'High', 'Low', 'Close', 'Volume']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis with Interactive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive price chart with fallback options\n",
    "print(\"Creating interactive price visualization...\")\n",
    "\n",
    "# Try Plotly first\n",
    "try:\n",
    "    # Interactive price chart with zoom capabilities\n",
    "    fig = make_subplots(rows=2, cols=1, \n",
    "                        shared_xaxes=True,\n",
    "                        subplot_titles=('Gold Futures Price (2020-2021)', 'Trading Volume'),\n",
    "                        vertical_spacing=0.1,\n",
    "                        row_heights=[0.7, 0.3])\n",
    "\n",
    "    # Add OHLC candlestick chart\n",
    "    fig.add_trace(\n",
    "        go.Candlestick(\n",
    "            x=data['Date'],\n",
    "            open=data['Open'],\n",
    "            high=data['High'],\n",
    "            low=data['Low'],\n",
    "            close=data['Close'],\n",
    "            name='Gold Futures',\n",
    "            increasing_line_color='gold',\n",
    "            decreasing_line_color='darkred'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add volume chart\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=data['Date'],\n",
    "            y=data['Volume'],\n",
    "            name='Volume',\n",
    "            marker_color='lightblue',\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Update layout for interactivity\n",
    "    fig.update_layout(\n",
    "        title='Gold Futures Interactive Analysis - 2020-2021',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Price ($)',\n",
    "        height=600,\n",
    "        showlegend=True,\n",
    "        xaxis_rangeslider_visible=False,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    # Add range selector buttons\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=3, label=\"3m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                    dict(step=\"all\")\n",
    "                ])\n",
    "            ),\n",
    "            rangeslider=dict(visible=True),\n",
    "            type=\"date\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    print(\"✅ Interactive Plotly chart created successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Plotly not available: {e}\")\n",
    "    print(\"Creating static matplotlib chart instead...\")\n",
    "    \n",
    "    # Fallback to matplotlib\n",
    "    try:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "        \n",
    "        # Price chart\n",
    "        ax1.plot(data['Date'], data['Close'], label='Close Price', color='gold', linewidth=2)\n",
    "        ax1.fill_between(data['Date'], data['Low'], data['High'], alpha=0.3, color='lightgray', label='High-Low Range')\n",
    "        ax1.set_title('Gold Futures Price (2020-2021)')\n",
    "        ax1.set_ylabel('Price ($)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Volume chart\n",
    "        ax2.bar(data['Date'], data['Volume'], alpha=0.7, color='lightblue')\n",
    "        ax2.set_title('Trading Volume')\n",
    "        ax2.set_xlabel('Date')\n",
    "        ax2.set_ylabel('Volume')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"✅ Static matplotlib chart created successfully!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Error creating charts: {e2}\")\n",
    "        print(\"Charts will be skipped, but analysis will continue...\")\n",
    "\n",
    "print(\"Chart creation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FEV Framework Setup and Custom Task Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FEV-compatible dataset for gold futures forecasting\n",
    "def create_fev_dataset(data):\n",
    "    \"\"\"\n",
    "    Convert gold futures data to FEV-compatible format\n",
    "    \"\"\"\n",
    "    # Create time series records\n",
    "    records = []\n",
    "    \n",
    "    # Use rolling windows for evaluation\n",
    "    window_size = 63  # 3 months of trading days\n",
    "    \n",
    "    for i in range(window_size, len(data)):\n",
    "        # Historical context\n",
    "        historical_data = data.iloc[i-window_size:i]\n",
    "        \n",
    "        # Target (next day close price)\n",
    "        target = data.iloc[i]['Close']\n",
    "        \n",
    "        record = {\n",
    "            'unique_id': f'gold_futures_{i}',\n",
    "            'ds': data.iloc[i]['Date'].strftime('%Y-%m-%d'),\n",
    "            'y': target,\n",
    "            'historical_data': historical_data['Close'].values.tolist(),\n",
    "            'context_length': window_size,\n",
    "            'prediction_length': 1\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    # Return different formats based on availability\n",
    "    if fev_available:\n",
    "        try:\n",
    "            from datasets import Dataset\n",
    "            return Dataset.from_list(records)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error creating HuggingFace dataset: {e}\")\n",
    "            print(\"Using list format instead\")\n",
    "            return records\n",
    "    else:\n",
    "        return records\n",
    "\n",
    "# Create FEV-compatible dataset\n",
    "print(\"Creating FEV-compatible dataset...\")\n",
    "fev_dataset = create_fev_dataset(data)\n",
    "\n",
    "if fev_available and hasattr(fev_dataset, '__len__'):\n",
    "    print(f\"✅ Created FEV dataset with {len(fev_dataset)} samples\")\n",
    "    print(f\"Each sample has {fev_dataset[0]['context_length']} historical data points\")\n",
    "    print(f\"Sample record keys: {list(fev_dataset[0].keys())}\")\n",
    "else:\n",
    "    print(f\"✅ Created dataset with {len(fev_dataset)} samples\")\n",
    "    print(f\"Each sample has {fev_dataset[0]['context_length']} historical data points\")\n",
    "    print(f\"Sample record keys: {list(fev_dataset[0].keys())}\")\n",
    "\n",
    "# Try to create FEV Task if available\n",
    "fev_task = None\n",
    "if fev_available:\n",
    "    try:\n",
    "        print(\"\\nAttempting to create FEV Task...\")\n",
    "        \n",
    "        # Create a simple FEV task\n",
    "        # Note: This may require specific dataset formats that FEV expects\n",
    "        # For now, we'll create a custom task-like object\n",
    "        \n",
    "        class FEVTask:\n",
    "            \"\"\"\n",
    "            FEV-compatible task for gold futures forecasting\n",
    "            \"\"\"\n",
    "            def __init__(self, dataset, context_length=63, prediction_length=1):\n",
    "                self.dataset = dataset\n",
    "                self.context_length = context_length\n",
    "                self.prediction_length = prediction_length\n",
    "                self.name = \"gold_futures_forecasting\"\n",
    "                self.target_column = \"y\"\n",
    "                self.horizon = prediction_length\n",
    "            \n",
    "            def get_input_data(self):\n",
    "                \"\"\"\n",
    "                Get input data in FEV format\n",
    "                \"\"\"\n",
    "                past_data = []\n",
    "                future_data = []\n",
    "                \n",
    "                for sample in self.dataset:\n",
    "                    past_data.append({\n",
    "                        'unique_id': sample['unique_id'],\n",
    "                        'ds': sample['ds'],\n",
    "                        'y': sample['historical_data']\n",
    "                    })\n",
    "                    future_data.append({\n",
    "                        'unique_id': sample['unique_id'],\n",
    "                        'ds': sample['ds'],\n",
    "                        'y': [sample['y']]\n",
    "                    })\n",
    "                \n",
    "                return past_data, future_data\n",
    "            \n",
    "            def evaluation_summary(self, predictions, model_name=\"model\"):\n",
    "                \"\"\"\n",
    "                Calculate evaluation metrics in FEV style\n",
    "                \"\"\"\n",
    "                if hasattr(self.dataset, '__getitem__'):\n",
    "                    actuals = [self.dataset[i]['y'] for i in range(len(self.dataset))]\n",
    "                else:\n",
    "                    actuals = [sample['y'] for sample in self.dataset]\n",
    "                \n",
    "                pred_values = [pred['predictions'][0] if isinstance(pred['predictions'], list) else pred['predictions'] \n",
    "                              for pred in predictions]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                mae = np.mean(np.abs(np.array(pred_values) - np.array(actuals)))\n",
    "                rmse = np.sqrt(np.mean((np.array(pred_values) - np.array(actuals)) ** 2))\n",
    "                mape = np.mean(np.abs((np.array(pred_values) - np.array(actuals)) / np.array(actuals))) * 100\n",
    "                \n",
    "                return {\n",
    "                    'model_name': model_name,\n",
    "                    'MAE': mae,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAPE': mape,\n",
    "                    'n_predictions': len(predictions)\n",
    "                }\n",
    "        \n",
    "        fev_task = FEVTask(fev_dataset)\n",
    "        print(f\"✅ Created FEV-compatible task: {fev_task.name}\")\n",
    "        print(f\"Task has {len(fev_task.dataset)} samples\")\n",
    "        print(f\"Context length: {fev_task.context_length}\")\n",
    "        print(f\"Prediction length: {fev_task.prediction_length}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating FEV task: {e}\")\n",
    "        print(\"Will use alternative evaluation approach\")\n",
    "        fev_task = None\n",
    "\n",
    "if not fev_available or fev_task is None:\n",
    "    print(\"\\n⚠️ Using alternative evaluation framework...\")\n",
    "    \n",
    "    # Create alternative task object\n",
    "    class AlternativeTask:\n",
    "        \"\"\"\n",
    "        Alternative task for gold futures forecasting when FEV is not available\n",
    "        \"\"\"\n",
    "        def __init__(self, dataset, context_length=63, prediction_length=1):\n",
    "            self.dataset = dataset\n",
    "            self.context_length = context_length\n",
    "            self.prediction_length = prediction_length\n",
    "            self.name = \"gold_futures_forecasting_alternative\"\n",
    "            self.target_column = \"y\"\n",
    "            self.horizon = prediction_length\n",
    "        \n",
    "        def get_input_data(self):\n",
    "            \"\"\"Get input data for evaluation\"\"\"\n",
    "            return [self.get_sample_data(i) for i in range(len(self.dataset))]\n",
    "        \n",
    "        def get_sample_data(self, idx):\n",
    "            \"\"\"Get input data for a specific sample\"\"\"\n",
    "            sample = self.dataset[idx] if hasattr(self.dataset, '__getitem__') else self.dataset[idx]\n",
    "            return {\n",
    "                'past_values': np.array(sample['historical_data']),\n",
    "                'future_values': np.array([sample['y']]),\n",
    "                'date': sample['ds'],\n",
    "                'unique_id': sample['unique_id']\n",
    "            }\n",
    "        \n",
    "        def evaluation_summary(self, predictions, model_name=\"model\"):\n",
    "            \"\"\"Calculate evaluation metrics\"\"\"\n",
    "            actuals = [sample['y'] for sample in self.dataset]\n",
    "            pred_values = [pred['predictions'][0] if isinstance(pred['predictions'], list) else pred['predictions'] \n",
    "                          for pred in predictions]\n",
    "            \n",
    "            mae = np.mean(np.abs(np.array(pred_values) - np.array(actuals)))\n",
    "            rmse = np.sqrt(np.mean((np.array(pred_values) - np.array(actuals)) ** 2))\n",
    "            mape = np.mean(np.abs((np.array(pred_values) - np.array(actuals)) / np.array(actuals))) * 100\n",
    "            \n",
    "            return {\n",
    "                'model_name': model_name,\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'n_predictions': len(predictions)\n",
    "            }\n",
    "    \n",
    "    fev_task = AlternativeTask(fev_dataset)\n",
    "    print(f\"✅ Created alternative task: {fev_task.name}\")\n",
    "\n",
    "print(f\"\\nEvaluation framework ready: {'FEV' if fev_available else 'Alternative'}\")\n",
    "print(f\"Total evaluation samples: {len(fev_task.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FEV-compatible evaluation task for gold futures forecasting\n",
    "print(\"Setting up evaluation task...\")\n",
    "\n",
    "# Create a comprehensive task object that works with or without FEV\n",
    "class GoldFuturesEvaluationTask:\n",
    "    \"\"\"\n",
    "    FEV-compatible task for gold futures forecasting evaluation\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, context_length=63, prediction_length=1, use_fev=False):\n",
    "        self.dataset = dataset\n",
    "        self.context_length = context_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.name = \"gold_futures_forecasting\"\n",
    "        self.target_column = \"y\"\n",
    "        self.horizon = prediction_length\n",
    "        self.use_fev = use_fev and fev_available\n",
    "        \n",
    "        print(f\"✅ Task created: {self.name}\")\n",
    "        print(f\"Framework: {'FEV' if self.use_fev else 'Custom'}\")\n",
    "        print(f\"Samples: {len(self.dataset)}\")\n",
    "        print(f\"Context length: {self.context_length}\")\n",
    "        print(f\"Prediction horizon: {self.prediction_length}\")\n",
    "    \n",
    "    def get_input_data(self, idx=None):\n",
    "        \"\"\"\n",
    "        Get input data for a specific sample or all samples\n",
    "        \"\"\"\n",
    "        if idx is not None:\n",
    "            # Get single sample\n",
    "            sample = self.dataset[idx] if hasattr(self.dataset, '__getitem__') else self.dataset[idx]\n",
    "            return {\n",
    "                'past_values': np.array(sample['historical_data']),\n",
    "                'future_values': np.array([sample['y']]),\n",
    "                'date': sample['ds'],\n",
    "                'unique_id': sample['unique_id']\n",
    "            }\n",
    "        else:\n",
    "            # Get all samples for FEV format\n",
    "            if self.use_fev:\n",
    "                past_data = []\n",
    "                future_data = []\n",
    "                \n",
    "                for sample in self.dataset:\n",
    "                    past_data.append({\n",
    "                        'unique_id': sample['unique_id'],\n",
    "                        'ds': sample['ds'],\n",
    "                        self.target_column: sample['historical_data']\n",
    "                    })\n",
    "                    future_data.append({\n",
    "                        'unique_id': sample['unique_id'],\n",
    "                        'ds': sample['ds'],\n",
    "                        self.target_column: [sample['y']]\n",
    "                    })\n",
    "                \n",
    "                return past_data, future_data\n",
    "            else:\n",
    "                # Return all samples for custom evaluation\n",
    "                return [self.get_input_data(i) for i in range(len(self.dataset))]\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        \"\"\"\n",
    "        Get all test data for evaluation\n",
    "        \"\"\"\n",
    "        return self.get_input_data()\n",
    "    \n",
    "    def evaluation_summary(self, predictions, model_name=\"model\"):\n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics compatible with FEV\n",
    "        \"\"\"\n",
    "        # Extract actual values\n",
    "        if hasattr(self.dataset, '__getitem__'):\n",
    "            actuals = [self.dataset[i]['y'] for i in range(len(self.dataset))]\n",
    "        else:\n",
    "            actuals = [sample['y'] for sample in self.dataset]\n",
    "        \n",
    "        # Extract predictions\n",
    "        if isinstance(predictions, list):\n",
    "            if len(predictions) > 0 and isinstance(predictions[0], dict):\n",
    "                # FEV-style predictions\n",
    "                pred_values = []\n",
    "                for pred in predictions:\n",
    "                    if 'predictions' in pred:\n",
    "                        val = pred['predictions']\n",
    "                        if isinstance(val, list):\n",
    "                            pred_values.append(val[0])\n",
    "                        else:\n",
    "                            pred_values.append(val)\n",
    "                    else:\n",
    "                        pred_values.append(list(pred.values())[0])\n",
    "            else:\n",
    "                # Simple list of predictions\n",
    "                pred_values = predictions\n",
    "        else:\n",
    "            pred_values = predictions\n",
    "        \n",
    "        # Ensure we have the right number of predictions\n",
    "        actuals = np.array(actuals[:len(pred_values)])\n",
    "        pred_values = np.array(pred_values[:len(actuals)])\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        mae = np.mean(np.abs(pred_values - actuals))\n",
    "        rmse = np.sqrt(np.mean((pred_values - actuals) ** 2))\n",
    "        mape = np.mean(np.abs((pred_values - actuals) / actuals)) * 100\n",
    "        \n",
    "        # Calculate MASE (Mean Absolute Scaled Error)\n",
    "        if len(actuals) > 1:\n",
    "            naive_forecast = actuals[:-1]\n",
    "            naive_mae = np.mean(np.abs(actuals[1:] - naive_forecast))\n",
    "            mase = mae / naive_mae if naive_mae > 0 else np.inf\n",
    "        else:\n",
    "            mase = np.inf\n",
    "        \n",
    "        # Directional accuracy\n",
    "        if len(actuals) > 1:\n",
    "            actual_direction = np.sign(np.diff(actuals))\n",
    "            pred_direction = np.sign(pred_values[1:] - actuals[:-1])\n",
    "            directional_accuracy = np.mean(actual_direction == pred_direction) * 100\n",
    "        else:\n",
    "            directional_accuracy = 50.0  # Random guess\n",
    "        \n",
    "        # Bias\n",
    "        bias = np.mean(pred_values - actuals)\n",
    "        \n",
    "        # R-squared\n",
    "        ss_res = np.sum((actuals - pred_values) ** 2)\n",
    "        ss_tot = np.sum((actuals - np.mean(actuals)) ** 2)\n",
    "        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "        \n",
    "        # FEV-compatible summary\n",
    "        summary = {\n",
    "            'model_name': model_name,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'MASE': mase,\n",
    "            'Directional_Accuracy': directional_accuracy,\n",
    "            'Bias': bias,\n",
    "            'R_Squared': r_squared,\n",
    "            'n_predictions': len(pred_values),\n",
    "            'Mean_Actual': np.mean(actuals),\n",
    "            'Mean_Prediction': np.mean(pred_values),\n",
    "            'Std_Actual': np.std(actuals),\n",
    "            'Std_Prediction': np.std(pred_values)\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def create_fev_predictions(self, model, max_samples=None):\n",
    "        \"\"\"\n",
    "        Create predictions in FEV format\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        n_samples = len(self.dataset)\n",
    "        if max_samples:\n",
    "            n_samples = min(n_samples, max_samples)\n",
    "        \n",
    "        print(f\"Generating predictions for {n_samples} samples...\")\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            if i % 20 == 0 and i > 0:\n",
    "                print(f\"Progress: {i}/{n_samples}\")\n",
    "            \n",
    "            try:\n",
    "                sample_data = self.get_input_data(i)\n",
    "                context = sample_data['past_values']\n",
    "                \n",
    "                # Generate prediction\n",
    "                if hasattr(model, 'predict_point'):\n",
    "                    pred = model.predict_point(context, self.prediction_length)\n",
    "                elif hasattr(model, 'predict'):\n",
    "                    pred = model.predict(context, self.prediction_length)\n",
    "                else:\n",
    "                    # Fallback to naive forecast\n",
    "                    pred = [context[-1]]\n",
    "                \n",
    "                # Format prediction\n",
    "                if isinstance(pred, np.ndarray):\n",
    "                    pred_value = pred[0]\n",
    "                elif isinstance(pred, list):\n",
    "                    pred_value = pred[0]\n",
    "                else:\n",
    "                    pred_value = pred\n",
    "                \n",
    "                predictions.append({\n",
    "                    'unique_id': sample_data['unique_id'],\n",
    "                    'predictions': pred_value\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error generating prediction for sample {i}: {e}\")\n",
    "                # Use naive forecast as fallback\n",
    "                sample_data = self.get_input_data(i)\n",
    "                predictions.append({\n",
    "                    'unique_id': sample_data['unique_id'],\n",
    "                    'predictions': sample_data['past_values'][-1]\n",
    "                })\n",
    "        \n",
    "        print(f\"✅ Generated {len(predictions)} predictions\")\n",
    "        return predictions\n",
    "\n",
    "# Create the evaluation task\n",
    "evaluation_task = GoldFuturesEvaluationTask(\n",
    "    dataset=fev_dataset,\n",
    "    context_length=63,\n",
    "    prediction_length=1,\n",
    "    use_fev=fev_available\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Evaluation task ready!\")\n",
    "print(f\"Task name: {evaluation_task.name}\")\n",
    "print(f\"Framework: {'FEV' if evaluation_task.use_fev else 'Custom'}\")\n",
    "print(f\"Total samples: {len(evaluation_task.dataset)}\")\n",
    "print(f\"Context window: {evaluation_task.context_length} days\")\n",
    "print(f\"Prediction horizon: {evaluation_task.prediction_length} day(s)\")\n",
    "\n",
    "# Test the task\n",
    "print(\"\\n🧪 Testing task functionality...\")\n",
    "try:\n",
    "    test_input = evaluation_task.get_input_data(0)\n",
    "    print(f\"✅ Sample data shape: {test_input['past_values'].shape}\")\n",
    "    print(f\"✅ Sample target: {test_input['future_values'][0]:.2f}\")\n",
    "    print(f\"✅ Sample date: {test_input['date']}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error testing task: {e}\")\n",
    "\n",
    "print(\"\\n🎯 Task setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chronos Model Setup and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Chronos-Bolt-Base model\n",
    "print(\"Loading Chronos-Bolt-Base model...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Load the model\n",
    "    chronos_pipeline = BaseChronosPipeline.from_pretrained(\n",
    "        \"amazon/chronos-bolt-base\",\n",
    "        device_map=device,\n",
    "        torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32\n",
    "    )\n",
    "    print(\"✅ Chronos-Bolt-Base model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading Chronos-Bolt-Base: {e}\")\n",
    "    print(\"Attempting to load alternative Chronos model...\")\n",
    "    \n",
    "    try:\n",
    "        # Try loading the smaller model\n",
    "        chronos_pipeline = BaseChronosPipeline.from_pretrained(\n",
    "            \"amazon/chronos-bolt-tiny\",\n",
    "            device_map=device,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        print(\"✅ Chronos-Bolt-Tiny model loaded successfully (fallback)!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Error loading Chronos-Bolt-Tiny: {e2}\")\n",
    "        print(\"Creating mock pipeline for demonstration...\")\n",
    "        \n",
    "        # Create a mock pipeline for demonstration\n",
    "        class MockChronosPipeline:\n",
    "            def __init__(self):\n",
    "                self.model_name = \"Mock Chronos Pipeline\"\n",
    "                \n",
    "            def predict_quantiles(self, context, prediction_length=1, quantile_levels=[0.1, 0.5, 0.9], num_samples=100):\n",
    "                # Simple naive forecast for demonstration\n",
    "                last_value = context[-1] if len(context) > 0 else 1800\n",
    "                \n",
    "                # Add some random variation\n",
    "                np.random.seed(42)\n",
    "                predictions = np.random.normal(last_value, last_value * 0.01, (1, prediction_length, len(quantile_levels)))\n",
    "                mean_pred = np.mean(predictions, axis=2, keepdims=True)\n",
    "                \n",
    "                return torch.tensor(predictions), torch.tensor(mean_pred)\n",
    "        \n",
    "        chronos_pipeline = MockChronosPipeline()\n",
    "        print(\"✅ Mock pipeline created for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chronos model wrapper for FEV compatibility\n",
    "class ChronosWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper to make Chronos compatible with FEV evaluation framework\n",
    "    \"\"\"\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.name = \"Chronos-Bolt-Base\"\n",
    "        \n",
    "        # Detect which type of Chronos pipeline we have\n",
    "        self.pipeline_type = type(pipeline).__name__\n",
    "        print(f\"Detected pipeline type: {self.pipeline_type}\")\n",
    "    \n",
    "    def predict(self, context, prediction_length=1, num_samples=100):\n",
    "        \"\"\"\n",
    "        Generate predictions using Chronos pipeline\n",
    "        \"\"\"\n",
    "        # Convert to tensor if needed\n",
    "        if isinstance(context, np.ndarray):\n",
    "            context_tensor = torch.tensor(context, dtype=torch.float32)\n",
    "        else:\n",
    "            context_tensor = context\n",
    "        \n",
    "        # Add batch dimension if needed\n",
    "        if len(context_tensor.shape) == 1:\n",
    "            context_tensor = context_tensor.unsqueeze(0)\n",
    "        \n",
    "        try:\n",
    "            # Try different prediction methods based on pipeline type\n",
    "            if hasattr(self.pipeline, 'predict_quantiles'):\n",
    "                # Check if it's a ChronosBolt pipeline first\n",
    "                if 'ChronosBolt' in self.pipeline_type:\n",
    "                    # ChronosBolt doesn't support num_samples parameter\n",
    "                    quantiles, mean = self.pipeline.predict_quantiles(\n",
    "                        context=context_tensor,\n",
    "                        prediction_length=prediction_length,\n",
    "                        quantile_levels=[0.1, 0.5, 0.9]\n",
    "                    )\n",
    "                else:\n",
    "                    # Regular Chronos pipeline supports num_samples\n",
    "                    quantiles, mean = self.pipeline.predict_quantiles(\n",
    "                        context=context_tensor,\n",
    "                        prediction_length=prediction_length,\n",
    "                        quantile_levels=[0.1, 0.5, 0.9],\n",
    "                        num_samples=num_samples\n",
    "                    )\n",
    "                \n",
    "                return {\n",
    "                    'mean': mean[0].cpu().numpy(),\n",
    "                    'quantiles': quantiles[0].cpu().numpy(),\n",
    "                    'q10': quantiles[0, :, 0].cpu().numpy(),\n",
    "                    'q50': quantiles[0, :, 1].cpu().numpy(),\n",
    "                    'q90': quantiles[0, :, 2].cpu().numpy()\n",
    "                }\n",
    "            \n",
    "            elif hasattr(self.pipeline, 'predict'):\n",
    "                # For other pipeline types\n",
    "                result = self.pipeline.predict(\n",
    "                    context=context_tensor,\n",
    "                    prediction_length=prediction_length\n",
    "                )\n",
    "                \n",
    "                if isinstance(result, tuple):\n",
    "                    mean = result[1] if len(result) > 1 else result[0]\n",
    "                    quantiles = result[0] if len(result) > 1 else result[0]\n",
    "                else:\n",
    "                    mean = result\n",
    "                    quantiles = result\n",
    "                \n",
    "                return {\n",
    "                    'mean': mean[0].cpu().numpy() if hasattr(mean, 'cpu') else mean[0],\n",
    "                    'quantiles': quantiles[0].cpu().numpy() if hasattr(quantiles, 'cpu') else quantiles[0]\n",
    "                }\n",
    "            \n",
    "            else:\n",
    "                # Fallback to simple prediction\n",
    "                print(\"⚠️ Using fallback prediction method\")\n",
    "                last_value = context_tensor[-1, -1].item()\n",
    "                return {\n",
    "                    'mean': np.array([last_value + np.random.normal(0, last_value * 0.01)]),\n",
    "                    'quantiles': np.array([[last_value * 0.99, last_value, last_value * 1.01]])\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in prediction: {e}\")\n",
    "            # Ultimate fallback\n",
    "            last_value = context_tensor[-1, -1].item()\n",
    "            return {\n",
    "                'mean': np.array([last_value]),\n",
    "                'quantiles': np.array([[last_value * 0.99, last_value, last_value * 1.01]])\n",
    "            }\n",
    "    \n",
    "    def predict_point(self, context, prediction_length=1):\n",
    "        \"\"\"\n",
    "        Generate point predictions (mean)\n",
    "        \"\"\"\n",
    "        result = self.predict(context, prediction_length)\n",
    "        return result['mean']\n",
    "\n",
    "# Create wrapper\n",
    "chronos_model = ChronosWrapper(chronos_pipeline)\n",
    "print(f\"Created Chronos wrapper: {chronos_model.name}\")\n",
    "\n",
    "# Test the wrapper\n",
    "print(\"Testing Chronos wrapper...\")\n",
    "try:\n",
    "    test_context = data['Close'].head(63).values\n",
    "    test_pred = chronos_model.predict_point(test_context)\n",
    "    actual_next = data['Close'].iloc[63]\n",
    "    \n",
    "    print(f\"✅ Test prediction: {test_pred[0]:.2f}\")\n",
    "    print(f\"✅ Actual next value: {actual_next:.2f}\")\n",
    "    print(f\"✅ Prediction error: {abs(test_pred[0] - actual_next):.2f}\")\n",
    "    \n",
    "    # Test full prediction with quantiles\n",
    "    full_pred = chronos_model.predict(test_context)\n",
    "    print(f\"✅ Quantile predictions: {full_pred['quantiles'][0]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error testing wrapper: {e}\")\n",
    "    print(\"Wrapper may still work with different inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Models Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement baseline models\n",
    "class BaselineModels:\n",
    "    \"\"\"\n",
    "    Collection of baseline forecasting models\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def naive_forecast(context, prediction_length=1):\n",
    "        \"\"\"\n",
    "        Naive forecast: use last observed value\n",
    "        \"\"\"\n",
    "        return np.full(prediction_length, context[-1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def seasonal_naive_forecast(context, prediction_length=1, season_length=5):\n",
    "        \"\"\"\n",
    "        Seasonal naive forecast: use value from same day of week\n",
    "        \"\"\"\n",
    "        if len(context) < season_length:\n",
    "            return BaselineModels.naive_forecast(context, prediction_length)\n",
    "        return np.full(prediction_length, context[-season_length])\n",
    "    \n",
    "    @staticmethod\n",
    "    def moving_average_forecast(context, prediction_length=1, window=5):\n",
    "        \"\"\"\n",
    "        Simple moving average forecast\n",
    "        \"\"\"\n",
    "        if len(context) < window:\n",
    "            window = len(context)\n",
    "        avg = np.mean(context[-window:])\n",
    "        return np.full(prediction_length, avg)\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear_trend_forecast(context, prediction_length=1):\n",
    "        \"\"\"\n",
    "        Linear trend forecast\n",
    "        \"\"\"\n",
    "        if len(context) < 2:\n",
    "            return BaselineModels.naive_forecast(context, prediction_length)\n",
    "        \n",
    "        x = np.arange(len(context))\n",
    "        slope, intercept = np.polyfit(x, context, 1)\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(prediction_length):\n",
    "            pred = slope * (len(context) + i) + intercept\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "# Create baseline model wrappers\n",
    "class BaselineWrapper:\n",
    "    def __init__(self, forecast_func, name):\n",
    "        self.forecast_func = forecast_func\n",
    "        self.name = name\n",
    "    \n",
    "    def predict_point(self, context, prediction_length=1):\n",
    "        return self.forecast_func(context, prediction_length)\n",
    "\n",
    "# Create baseline models\n",
    "baseline_models = {\n",
    "    'Naive': BaselineWrapper(BaselineModels.naive_forecast, 'Naive'),\n",
    "    'Seasonal_Naive': BaselineWrapper(BaselineModels.seasonal_naive_forecast, 'Seasonal Naive'),\n",
    "    'Moving_Average': BaselineWrapper(BaselineModels.moving_average_forecast, 'Moving Average (5-day)'),\n",
    "    'Linear_Trend': BaselineWrapper(BaselineModels.linear_trend_forecast, 'Linear Trend')\n",
    "}\n",
    "\n",
    "print(\"Baseline models created:\")\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"  - {model.name}\")\n",
    "\n",
    "# Test baseline models\n",
    "test_context = data['Close'].head(63).values\n",
    "print(\"\\nTest predictions:\")\n",
    "for name, model in baseline_models.items():\n",
    "    pred = model.predict_point(test_context)\n",
    "    print(f\"  {model.name}: {pred[0]:.2f}\")\n",
    "print(f\"  Actual: {data['Close'].iloc[63]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Rolling Window Evaluation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement rolling window evaluation\n",
    "def rolling_window_evaluation(data, models, window_size=63, start_idx=None, end_idx=None):\n",
    "    \"\"\"\n",
    "    Perform rolling window evaluation on multiple models\n",
    "    \"\"\"\n",
    "    if start_idx is None:\n",
    "        start_idx = window_size\n",
    "    if end_idx is None:\n",
    "        end_idx = len(data)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Initialize results storage\n",
    "    for model_name in models.keys():\n",
    "        results[model_name] = {\n",
    "            'predictions': [],\n",
    "            'actuals': [],\n",
    "            'dates': [],\n",
    "            'errors': [],\n",
    "            'relative_errors': []\n",
    "        }\n",
    "    \n",
    "    print(f\"Starting rolling window evaluation from index {start_idx} to {end_idx}\")\n",
    "    print(f\"Total predictions to generate: {end_idx - start_idx}\")\n",
    "    \n",
    "    # Rolling window loop\n",
    "    for i in range(start_idx, end_idx):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Progress: {i - start_idx + 1}/{end_idx - start_idx} predictions\")\n",
    "        \n",
    "        # Get context window\n",
    "        context = data['Close'].iloc[i-window_size:i].values\n",
    "        actual = data['Close'].iloc[i]\n",
    "        date = data['Date'].iloc[i]\n",
    "        \n",
    "        # Generate predictions for all models\n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                pred = model.predict_point(context, prediction_length=1)\n",
    "                prediction = pred[0] if isinstance(pred, np.ndarray) else pred\n",
    "                \n",
    "                # Calculate error metrics\n",
    "                error = abs(actual - prediction)\n",
    "                relative_error = error / actual * 100\n",
    "                \n",
    "                # Store results\n",
    "                results[model_name]['predictions'].append(prediction)\n",
    "                results[model_name]['actuals'].append(actual)\n",
    "                results[model_name]['dates'].append(date)\n",
    "                results[model_name]['errors'].append(error)\n",
    "                results[model_name]['relative_errors'].append(relative_error)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with {model_name} at index {i}: {e}\")\n",
    "                # Use naive forecast as fallback\n",
    "                prediction = context[-1]\n",
    "                error = abs(actual - prediction)\n",
    "                relative_error = error / actual * 100\n",
    "                \n",
    "                results[model_name]['predictions'].append(prediction)\n",
    "                results[model_name]['actuals'].append(actual)\n",
    "                results[model_name]['dates'].append(date)\n",
    "                results[model_name]['errors'].append(error)\n",
    "                results[model_name]['relative_errors'].append(relative_error)\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    for model_name in results.keys():\n",
    "        for key in ['predictions', 'actuals', 'errors', 'relative_errors']:\n",
    "            results[model_name][key] = np.array(results[model_name][key])\n",
    "    \n",
    "    print(\"Rolling window evaluation completed!\")\n",
    "    return results\n",
    "\n",
    "# Combine all models for evaluation\n",
    "all_models = {**baseline_models, 'Chronos': chronos_model}\n",
    "\n",
    "print(f\"Models to evaluate: {list(all_models.keys())}\")\n",
    "print(f\"Starting evaluation on {len(data)} data points...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation (this may take some time)\n",
    "print(\"Running rolling window evaluation...\")\n",
    "print(\"This may take several minutes depending on your hardware.\")\n",
    "\n",
    "# Run evaluation on a subset first for testing\n",
    "test_results = rolling_window_evaluation(\n",
    "    data=data,\n",
    "    models=all_models,\n",
    "    window_size=63,\n",
    "    start_idx=63,\n",
    "    end_idx=min(163, len(data))  # First 100 predictions for testing\n",
    ")\n",
    "\n",
    "print(\"\\nTest evaluation completed!\")\n",
    "print(f\"Generated {len(test_results['Naive']['predictions'])} predictions per model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Metrics and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive performance metrics\n",
    "def calculate_metrics(results):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for all models\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for model_name, model_results in results.items():\n",
    "        predictions = model_results['predictions']\n",
    "        actuals = model_results['actuals']\n",
    "        \n",
    "        # Basic metrics\n",
    "        mae = np.mean(np.abs(predictions - actuals))\n",
    "        rmse = np.sqrt(np.mean((predictions - actuals) ** 2))\n",
    "        mape = np.mean(np.abs((predictions - actuals) / actuals)) * 100\n",
    "        \n",
    "        # MASE (Mean Absolute Scaled Error)\n",
    "        naive_forecast = actuals[:-1]\n",
    "        naive_mae = np.mean(np.abs(actuals[1:] - naive_forecast))\n",
    "        mase = mae / naive_mae if naive_mae > 0 else np.inf\n",
    "        \n",
    "        # Directional accuracy\n",
    "        actual_direction = np.sign(np.diff(actuals))\n",
    "        pred_direction = np.sign(predictions[1:] - actuals[:-1])\n",
    "        directional_accuracy = np.mean(actual_direction == pred_direction) * 100\n",
    "        \n",
    "        # Bias\n",
    "        bias = np.mean(predictions - actuals)\n",
    "        \n",
    "        # R-squared\n",
    "        ss_res = np.sum((actuals - predictions) ** 2)\n",
    "        ss_tot = np.sum((actuals - np.mean(actuals)) ** 2)\n",
    "        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "        \n",
    "        metrics[model_name] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'MASE': mase,\n",
    "            'Directional_Accuracy': directional_accuracy,\n",
    "            'Bias': bias,\n",
    "            'R_Squared': r_squared,\n",
    "            'N_Predictions': len(predictions)\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for test results\n",
    "test_metrics = calculate_metrics(test_results)\n",
    "\n",
    "# Create metrics DataFrame\n",
    "metrics_df = pd.DataFrame(test_metrics).T\n",
    "metrics_df = metrics_df.round(4)\n",
    "\n",
    "print(\"Performance Metrics (Test Set):\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Rank models by MASE (lower is better)\n",
    "ranking = metrics_df.sort_values('MASE')\n",
    "print(\"\\nModel Ranking by MASE (lower is better):\")\n",
    "for i, (model, row) in enumerate(ranking.iterrows(), 1):\n",
    "    print(f\"{i}. {model}: MASE = {row['MASE']:.4f}, Dir. Acc. = {row['Directional_Accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive prediction visualization\n",
    "def create_prediction_dashboard(results, metrics):\n",
    "    \"\"\"\n",
    "    Create interactive dashboard for prediction results\n",
    "    \"\"\"\n",
    "    if plotly_available:\n",
    "        # Create subplot with secondary y-axis\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Actual vs Predicted Prices', 'Prediction Errors Over Time',\n",
    "                'Model Performance Comparison', 'Error Distribution',\n",
    "                'Directional Accuracy', 'Rolling Performance'\n",
    "            ),\n",
    "            specs=[\n",
    "                [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                [{\"secondary_y\": False}, {\"secondary_y\": False}]\n",
    "            ],\n",
    "            vertical_spacing=0.08,\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # Colors for different models\n",
    "        colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "        \n",
    "        # Plot 1: Actual vs Predicted\n",
    "        # Add actual prices\n",
    "        actual_dates = results['Naive']['dates']  # Use dates from any model\n",
    "        actual_prices = results['Naive']['actuals']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=actual_dates,\n",
    "                y=actual_prices,\n",
    "                mode='lines',\n",
    "                name='Actual',\n",
    "                line=dict(color='black', width=2),\n",
    "                hovertemplate='Date: %{x}<br>Actual: $%{y:.2f}<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add predictions for each model\n",
    "        for i, (model_name, model_results) in enumerate(results.items()):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=model_results['dates'],\n",
    "                    y=model_results['predictions'],\n",
    "                    mode='lines',\n",
    "                    name=f'{model_name} Pred',\n",
    "                    line=dict(color=colors[i % len(colors)], width=1, dash='dash'),\n",
    "                    hovertemplate=f'{model_name}: $%{{y:.2f}}<extra></extra>'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Plot 2: Prediction Errors Over Time\n",
    "        for i, (model_name, model_results) in enumerate(results.items()):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=model_results['dates'],\n",
    "                    y=model_results['errors'],\n",
    "                    mode='lines',\n",
    "                    name=f'{model_name} Error',\n",
    "                    line=dict(color=colors[i % len(colors)]),\n",
    "                    showlegend=False,\n",
    "                    hovertemplate=f'{model_name} Error: $%{{y:.2f}}<extra></extra>'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # Plot 3: Model Performance Comparison (Bar chart)\n",
    "        model_names = list(metrics.keys())\n",
    "        mase_values = [metrics[name]['MASE'] for name in model_names]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=model_names,\n",
    "                y=mase_values,\n",
    "                name='MASE',\n",
    "                marker_color=colors[:len(model_names)],\n",
    "                showlegend=False,\n",
    "                hovertemplate='Model: %{x}<br>MASE: %{y:.4f}<extra></extra>'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Plot 4: Error Distribution\n",
    "        for i, (model_name, model_results) in enumerate(results.items()):\n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=model_results['errors'],\n",
    "                    name=f'{model_name} Errors',\n",
    "                    opacity=0.7,\n",
    "                    marker_color=colors[i % len(colors)],\n",
    "                    showlegend=False,\n",
    "                    hovertemplate=f'{model_name}<br>Error: $%{{x:.2f}}<br>Count: %{{y}}<extra></extra>'\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        # Plot 5: Directional Accuracy\n",
    "        dir_acc_values = [metrics[name]['Directional_Accuracy'] for name in model_names]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=model_names,\n",
    "                y=dir_acc_values,\n",
    "                name='Directional Accuracy',\n",
    "                marker_color=colors[:len(model_names)],\n",
    "                showlegend=False,\n",
    "                hovertemplate='Model: %{x}<br>Directional Accuracy: %{y:.2f}%<extra></extra>'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        # Plot 6: Rolling Performance (Rolling MAE)\n",
    "        window_size = 10\n",
    "        for i, (model_name, model_results) in enumerate(results.items()):\n",
    "            if len(model_results['errors']) >= window_size:\n",
    "                rolling_mae = pd.Series(model_results['errors']).rolling(window=window_size).mean()\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=model_results['dates'],\n",
    "                        y=rolling_mae,\n",
    "                        mode='lines',\n",
    "                        name=f'{model_name} Rolling MAE',\n",
    "                        line=dict(color=colors[i % len(colors)]),\n",
    "                        showlegend=False,\n",
    "                        hovertemplate=f'{model_name} Rolling MAE: $%{{y:.2f}}<extra></extra>'\n",
    "                    ),\n",
    "                    row=3, col=2\n",
    "                )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title='Gold Futures Forecasting - Interactive Performance Dashboard',\n",
    "            height=1200,\n",
    "            showlegend=True,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Update axes labels\n",
    "        fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Price ($)\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Date\", row=1, col=2)\n",
    "        fig.update_yaxes(title_text=\"Error ($)\", row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"Model\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"MASE\", row=2, col=1)\n",
    "        fig.update_xaxes(title_text=\"Error ($)\", row=2, col=2)\n",
    "        fig.update_yaxes(title_text=\"Frequency\", row=2, col=2)\n",
    "        fig.update_xaxes(title_text=\"Model\", row=3, col=1)\n",
    "        fig.update_yaxes(title_text=\"Directional Accuracy (%)\", row=3, col=1)\n",
    "        fig.update_xaxes(title_text=\"Date\", row=3, col=2)\n",
    "        fig.update_yaxes(title_text=\"Rolling MAE ($)\", row=3, col=2)\n",
    "        \n",
    "        return fig\n",
    "    else:\n",
    "        # Return None if Plotly not available\n",
    "        return None\n",
    "\n",
    "# Create the dashboard\n",
    "if plotly_available:\n",
    "    print(\"Creating interactive Plotly dashboard...\")\n",
    "    dashboard = create_prediction_dashboard(test_results, test_metrics)\n",
    "    \n",
    "    try:\n",
    "        dashboard.show()\n",
    "        print(\"✅ Interactive Plotly dashboard created successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Plotly dashboard display failed: {e}\")\n",
    "        plotly_available = False  # Fall back to matplotlib\n",
    "\n",
    "if not plotly_available:\n",
    "    print(\"Creating static matplotlib dashboard...\")\n",
    "    \n",
    "    # Create static matplotlib dashboard as fallback\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Gold Futures Forecasting - Performance Dashboard', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted\n",
    "    ax1 = axes[0, 0]\n",
    "    actual_dates = test_results['Naive']['dates']\n",
    "    actual_prices = test_results['Naive']['actuals']\n",
    "    \n",
    "    ax1.plot(actual_dates, actual_prices, 'k-', linewidth=2, label='Actual')\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    for i, (model_name, model_results) in enumerate(test_results.items()):\n",
    "        ax1.plot(model_results['dates'], model_results['predictions'], \n",
    "                '--', color=colors[i % len(colors)], label=f'{model_name}', alpha=0.7)\n",
    "    ax1.set_title('Actual vs Predicted Prices')\n",
    "    ax1.set_ylabel('Price ($)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Model Performance (MASE)\n",
    "    ax2 = axes[0, 1]\n",
    "    model_names = list(test_metrics.keys())\n",
    "    mase_values = [test_metrics[name]['MASE'] for name in model_names]\n",
    "    bars = ax2.bar(model_names, mase_values, color=colors[:len(model_names)])\n",
    "    ax2.set_title('Model Performance (MASE)')\n",
    "    ax2.set_ylabel('MASE')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, mase_values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 3: Directional Accuracy\n",
    "    ax3 = axes[0, 2]\n",
    "    dir_acc_values = [test_metrics[name]['Directional_Accuracy'] for name in model_names]\n",
    "    bars = ax3.bar(model_names, dir_acc_values, color=colors[:len(model_names)])\n",
    "    ax3.set_title('Directional Accuracy (%)')\n",
    "    ax3.set_ylabel('Directional Accuracy (%)')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, dir_acc_values):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{value:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 4: Error Distribution for Chronos\n",
    "    ax4 = axes[1, 0]\n",
    "    chronos_errors = test_results['Chronos']['errors']\n",
    "    ax4.hist(chronos_errors, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "    ax4.set_title('Chronos Error Distribution')\n",
    "    ax4.set_xlabel('Error ($)')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Prediction Errors Over Time\n",
    "    ax5 = axes[1, 1]\n",
    "    for i, (model_name, model_results) in enumerate(test_results.items()):\n",
    "        ax5.plot(model_results['dates'], model_results['errors'], \n",
    "                label=f'{model_name}', color=colors[i % len(colors)], alpha=0.7)\n",
    "    ax5.set_title('Prediction Errors Over Time')\n",
    "    ax5.set_xlabel('Date')\n",
    "    ax5.set_ylabel('Error ($)')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    ax5.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 6: RMSE Comparison\n",
    "    ax6 = axes[1, 2]\n",
    "    rmse_values = [test_metrics[name]['RMSE'] for name in model_names]\n",
    "    bars = ax6.bar(model_names, rmse_values, color=colors[:len(model_names)])\n",
    "    ax6.set_title('RMSE Comparison')\n",
    "    ax6.set_ylabel('RMSE')\n",
    "    ax6.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, rmse_values):\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"✅ Static matplotlib dashboard created successfully!\")\n",
    "\n",
    "print(\"Dashboard creation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Statistical Analysis and Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical significance tests\n",
    "def diebold_mariano_test(actual, pred1, pred2):\n",
    "    \"\"\"\n",
    "    Diebold-Mariano test for forecast accuracy comparison\n",
    "    \"\"\"\n",
    "    # Calculate forecast errors\n",
    "    e1 = actual - pred1\n",
    "    e2 = actual - pred2\n",
    "    \n",
    "    # Calculate loss differential\n",
    "    d = np.abs(e1) - np.abs(e2)\n",
    "    \n",
    "    # Test statistic\n",
    "    dbar = np.mean(d)\n",
    "    gamma0 = np.var(d, ddof=1)\n",
    "    \n",
    "    if gamma0 == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Calculate test statistic\n",
    "    n = len(d)\n",
    "    dm_stat = dbar / np.sqrt(gamma0 / n)\n",
    "    \n",
    "    # Calculate p-value (two-tailed test)\n",
    "    p_value = 2 * (1 - stats.norm.cdf(np.abs(dm_stat)))\n",
    "    \n",
    "    return dm_stat, p_value\n",
    "\n",
    "# Perform pairwise comparisons\n",
    "def perform_significance_tests(results):\n",
    "    \"\"\"\n",
    "    Perform pairwise significance tests between all models\n",
    "    \"\"\"\n",
    "    model_names = list(results.keys())\n",
    "    n_models = len(model_names)\n",
    "    \n",
    "    # Create matrices for test statistics and p-values\n",
    "    dm_stats = np.zeros((n_models, n_models))\n",
    "    p_values = np.zeros((n_models, n_models))\n",
    "    \n",
    "    # Get actual values (same for all models)\n",
    "    actual = results[model_names[0]]['actuals']\n",
    "    \n",
    "    # Perform pairwise tests\n",
    "    for i in range(n_models):\n",
    "        for j in range(n_models):\n",
    "            if i != j:\n",
    "                pred1 = results[model_names[i]]['predictions']\n",
    "                pred2 = results[model_names[j]]['predictions']\n",
    "                \n",
    "                dm_stat, p_val = diebold_mariano_test(actual, pred1, pred2)\n",
    "                dm_stats[i, j] = dm_stat\n",
    "                p_values[i, j] = p_val\n",
    "    \n",
    "    return dm_stats, p_values, model_names\n",
    "\n",
    "# Perform significance tests\n",
    "dm_stats, p_values, model_names = perform_significance_tests(test_results)\n",
    "\n",
    "# Create significance test results DataFrame\n",
    "significance_df = pd.DataFrame(p_values, index=model_names, columns=model_names)\n",
    "significance_df = significance_df.round(4)\n",
    "\n",
    "print(\"Statistical Significance Tests (Diebold-Mariano):\")\n",
    "print(\"P-values for pairwise comparisons (H0: Equal forecast accuracy)\")\n",
    "print(significance_df)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- p < 0.05: Significant difference in forecast accuracy\")\n",
    "print(\"- p >= 0.05: No significant difference in forecast accuracy\")\n",
    "\n",
    "# Find best performing model pairs\n",
    "print(\"\\nSignificant differences (p < 0.05):\")\n",
    "for i, model1 in enumerate(model_names):\n",
    "    for j, model2 in enumerate(model_names):\n",
    "        if i != j and p_values[i, j] < 0.05:\n",
    "            direction = \"better\" if dm_stats[i, j] < 0 else \"worse\"\n",
    "            print(f\"  {model1} is significantly {direction} than {model2} (p = {p_values[i, j]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Full Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation on complete dataset\n",
    "print(\"Running full dataset evaluation...\")\n",
    "print(\"This will take significantly longer. Please be patient.\")\n",
    "\n",
    "# Option to run full evaluation or use test results\n",
    "run_full_evaluation = True  # Set to False to skip full evaluation\n",
    "\n",
    "if run_full_evaluation:\n",
    "    # Run full evaluation\n",
    "    full_results = rolling_window_evaluation(\n",
    "        data=data,\n",
    "        models=all_models,\n",
    "        window_size=63,\n",
    "        start_idx=63,\n",
    "        end_idx=len(data)\n",
    "    )\n",
    "    \n",
    "    # Calculate full metrics\n",
    "    full_metrics = calculate_metrics(full_results)\n",
    "    \n",
    "    # Create full metrics DataFrame\n",
    "    full_metrics_df = pd.DataFrame(full_metrics).T\n",
    "    full_metrics_df = full_metrics_df.round(4)\n",
    "    \n",
    "    print(\"\\nFull Dataset Performance Metrics:\")\n",
    "    print(full_metrics_df)\n",
    "    \n",
    "    # Save results for later use\n",
    "    results_to_use = full_results\n",
    "    metrics_to_use = full_metrics\n",
    "    \n",
    "else:\n",
    "    print(\"Using test results for demonstration...\")\n",
    "    results_to_use = test_results\n",
    "    metrics_to_use = test_metrics\n",
    "\n",
    "print(f\"\\nTotal predictions generated: {len(results_to_use['Naive']['predictions'])}\")\n",
    "print(f\"Evaluation period: {results_to_use['Naive']['dates'][0]} to {results_to_use['Naive']['dates'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Results and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final results summary\n",
    "def create_final_summary(metrics, results):\n",
    "    \"\"\"\n",
    "    Create comprehensive final summary\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Model ranking\n",
    "    metrics_df = pd.DataFrame(metrics).T\n",
    "    ranking = metrics_df.sort_values('MASE')\n",
    "    \n",
    "    print(\"\\n📊 MODEL RANKING (by MASE - Lower is Better):\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (model, row) in enumerate(ranking.iterrows(), 1):\n",
    "        print(f\"{i}. {model:20s} | MASE: {row['MASE']:6.4f} | Dir.Acc: {row['Directional_Accuracy']:5.1f}% | RMSE: {row['RMSE']:6.2f}\")\n",
    "    \n",
    "    # Best model analysis\n",
    "    best_model = ranking.index[0]\n",
    "    best_metrics = ranking.iloc[0]\n",
    "    \n",
    "    print(f\"\\n🏆 BEST PERFORMING MODEL: {best_model}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Mean Absolute Scaled Error (MASE): {best_metrics['MASE']:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {best_metrics['MAE']:.2f}\")\n",
    "    print(f\"Root Mean Square Error (RMSE): {best_metrics['RMSE']:.2f}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {best_metrics['MAPE']:.2f}%\")\n",
    "    print(f\"Directional Accuracy: {best_metrics['Directional_Accuracy']:.2f}%\")\n",
    "    print(f\"R-squared: {best_metrics['R_Squared']:.4f}\")\n",
    "    \n",
    "    # Chronos-specific analysis\n",
    "    if 'Chronos' in metrics:\n",
    "        chronos_rank = list(ranking.index).index('Chronos') + 1\n",
    "        chronos_metrics = metrics['Chronos']\n",
    "        \n",
    "        print(f\"\\n🤖 CHRONOS-BOLT-BASE PERFORMANCE:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Rank: {chronos_rank} out of {len(metrics)} models\")\n",
    "        print(f\"MASE: {chronos_metrics['MASE']:.4f}\")\n",
    "        print(f\"Directional Accuracy: {chronos_metrics['Directional_Accuracy']:.2f}%\")\n",
    "        print(f\"RMSE: {chronos_metrics['RMSE']:.2f}\")\n",
    "        \n",
    "        # Compare with naive baseline\n",
    "        if 'Naive' in metrics:\n",
    "            naive_mase = metrics['Naive']['MASE']\n",
    "            improvement = (naive_mase - chronos_metrics['MASE']) / naive_mase * 100\n",
    "            print(f\"Improvement over Naive: {improvement:.1f}%\")\n",
    "    \n",
    "    # Data insights\n",
    "    total_predictions = len(results[list(results.keys())[0]]['predictions'])\n",
    "    date_range = f\"{results[list(results.keys())[0]]['dates'][0]} to {results[list(results.keys())[0]]['dates'][-1]}\"\n",
    "    \n",
    "    print(f\"\\n📈 EVALUATION SUMMARY:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total Predictions: {total_predictions}\")\n",
    "    print(f\"Evaluation Period: {date_range}\")\n",
    "    print(f\"Context Window: 63 trading days (3 months)\")\n",
    "    print(f\"Prediction Horizon: 1 day ahead\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if best_model == 'Chronos':\n",
    "        print(\"✅ Chronos-Bolt-Base shows superior performance for gold futures forecasting\")\n",
    "        print(\"✅ Consider using Chronos for production forecasting systems\")\n",
    "    else:\n",
    "        print(f\"⚠️  {best_model} outperforms Chronos-Bolt-Base on this dataset\")\n",
    "        print(\"⚠️  Consider ensemble methods or parameter tuning for Chronos\")\n",
    "    \n",
    "    print(\"📋 Additional considerations:\")\n",
    "    print(\"   - Evaluate performance across different market conditions\")\n",
    "    print(\"   - Test with different prediction horizons\")\n",
    "    print(\"   - Consider transaction costs in real trading scenarios\")\n",
    "    print(\"   - Validate on out-of-sample data from different time periods\")\n",
    "    \n",
    "    return ranking\n",
    "\n",
    "# Generate final summary\n",
    "final_ranking = create_final_summary(metrics_to_use, results_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Results and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to files\n",
    "def export_results(metrics, results, filename_prefix=\"gold_futures_forecast\"):\n",
    "    \"\"\"\n",
    "    Export results to CSV and HTML files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Export metrics\n",
    "        metrics_df = pd.DataFrame(metrics).T\n",
    "        metrics_df.to_csv(f\"{filename_prefix}_metrics.csv\")\n",
    "        print(f\"✅ Metrics exported to {filename_prefix}_metrics.csv\")\n",
    "        \n",
    "        # Export predictions\n",
    "        predictions_data = []\n",
    "        for model_name, model_results in results.items():\n",
    "            for i, date in enumerate(model_results['dates']):\n",
    "                predictions_data.append({\n",
    "                    'Date': date,\n",
    "                    'Model': model_name,\n",
    "                    'Actual': model_results['actuals'][i],\n",
    "                    'Predicted': model_results['predictions'][i],\n",
    "                    'Error': model_results['errors'][i],\n",
    "                    'Relative_Error': model_results['relative_errors'][i]\n",
    "                })\n",
    "        \n",
    "        predictions_df = pd.DataFrame(predictions_data)\n",
    "        predictions_df.to_csv(f\"{filename_prefix}_predictions.csv\", index=False)\n",
    "        print(f\"✅ Predictions exported to {filename_prefix}_predictions.csv\")\n",
    "        \n",
    "        # Export dashboard as HTML (try both Plotly and matplotlib)\n",
    "        try:\n",
    "            if plotly_available:\n",
    "                dashboard_fig = create_prediction_dashboard(results, metrics)\n",
    "                if dashboard_fig is not None:\n",
    "                    dashboard_fig.write_html(f\"{filename_prefix}_dashboard.html\")\n",
    "                    print(f\"✅ Interactive Plotly dashboard exported to {filename_prefix}_dashboard.html\")\n",
    "                else:\n",
    "                    print(\"⚠️ Plotly dashboard not available, creating matplotlib export...\")\n",
    "                    raise Exception(\"Plotly not available\")\n",
    "            else:\n",
    "                raise Exception(\"Plotly not available\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Plotly export failed ({e}), creating matplotlib report...\")\n",
    "            \n",
    "            # Create a comprehensive matplotlib report\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(20, 14))\n",
    "            fig.suptitle('Gold Futures Forecasting - Comprehensive Report', fontsize=18, fontweight='bold')\n",
    "            \n",
    "            # Plot 1: Actual vs Predicted\n",
    "            ax1 = axes[0, 0]\n",
    "            actual_dates = results['Naive']['dates']\n",
    "            actual_prices = results['Naive']['actuals']\n",
    "            \n",
    "            ax1.plot(actual_dates, actual_prices, 'k-', linewidth=3, label='Actual', alpha=0.8)\n",
    "            colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "            for i, (model_name, model_results) in enumerate(results.items()):\n",
    "                ax1.plot(model_results['dates'], model_results['predictions'], \n",
    "                        '--', color=colors[i % len(colors)], label=f'{model_name}', \n",
    "                        linewidth=2, alpha=0.8)\n",
    "            ax1.set_title('Actual vs Predicted Prices', fontsize=14, fontweight='bold')\n",
    "            ax1.set_ylabel('Price ($)', fontsize=12)\n",
    "            ax1.legend(fontsize=10)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Plot 2: Model Performance (MASE)\n",
    "            ax2 = axes[0, 1]\n",
    "            model_names = list(metrics.keys())\n",
    "            mase_values = [metrics[name]['MASE'] for name in model_names]\n",
    "            bars = ax2.bar(model_names, mase_values, color=colors[:len(model_names)], alpha=0.8)\n",
    "            ax2.set_title('Model Performance (MASE)', fontsize=14, fontweight='bold')\n",
    "            ax2.set_ylabel('MASE (Lower is Better)', fontsize=12)\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, value in zip(bars, mase_values):\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                        f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            # Plot 3: Directional Accuracy\n",
    "            ax3 = axes[0, 2]\n",
    "            dir_acc_values = [metrics[name]['Directional_Accuracy'] for name in model_names]\n",
    "            bars = ax3.bar(model_names, dir_acc_values, color=colors[:len(model_names)], alpha=0.8)\n",
    "            ax3.set_title('Directional Accuracy', fontsize=14, fontweight='bold')\n",
    "            ax3.set_ylabel('Directional Accuracy (%)', fontsize=12)\n",
    "            ax3.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, value in zip(bars, dir_acc_values):\n",
    "                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                        f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            # Plot 4: Model Comparison Table (as text)\n",
    "            ax4 = axes[1, 0]\n",
    "            ax4.axis('off')\n",
    "            \n",
    "            # Create performance table\n",
    "            table_data = []\n",
    "            for model in model_names:\n",
    "                table_data.append([\n",
    "                    model,\n",
    "                    f\"{metrics[model]['MASE']:.4f}\",\n",
    "                    f\"{metrics[model]['MAE']:.2f}\",\n",
    "                    f\"{metrics[model]['RMSE']:.2f}\",\n",
    "                    f\"{metrics[model]['MAPE']:.2f}%\",\n",
    "                    f\"{metrics[model]['Directional_Accuracy']:.1f}%\"\n",
    "                ])\n",
    "            \n",
    "            table = ax4.table(cellText=table_data,\n",
    "                            colLabels=['Model', 'MASE', 'MAE', 'RMSE', 'MAPE', 'Dir.Acc'],\n",
    "                            cellLoc='center',\n",
    "                            loc='center',\n",
    "                            bbox=[0, 0, 1, 1])\n",
    "            table.auto_set_font_size(False)\n",
    "            table.set_fontsize(9)\n",
    "            table.scale(1, 2)\n",
    "            ax4.set_title('Performance Summary Table', fontsize=14, fontweight='bold', pad=20)\n",
    "            \n",
    "            # Plot 5: Prediction Errors Over Time\n",
    "            ax5 = axes[1, 1]\n",
    "            for i, (model_name, model_results) in enumerate(results.items()):\n",
    "                ax5.plot(model_results['dates'], model_results['errors'], \n",
    "                        label=f'{model_name}', color=colors[i % len(colors)], \n",
    "                        linewidth=2, alpha=0.8)\n",
    "            ax5.set_title('Prediction Errors Over Time', fontsize=14, fontweight='bold')\n",
    "            ax5.set_xlabel('Date', fontsize=12)\n",
    "            ax5.set_ylabel('Error ($)', fontsize=12)\n",
    "            ax5.legend(fontsize=10)\n",
    "            ax5.grid(True, alpha=0.3)\n",
    "            ax5.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Plot 6: Error Distribution for Best Model (Chronos)\n",
    "            ax6 = axes[1, 2]\n",
    "            if 'Chronos' in results:\n",
    "                chronos_errors = results['Chronos']['errors']\n",
    "                ax6.hist(chronos_errors, bins=15, alpha=0.8, color='purple', \n",
    "                        edgecolor='black', linewidth=1.2)\n",
    "                ax6.axvline(np.mean(chronos_errors), color='red', linestyle='--', \n",
    "                           linewidth=2, label=f'Mean: ${np.mean(chronos_errors):.2f}')\n",
    "                ax6.set_title('Chronos Error Distribution', fontsize=14, fontweight='bold')\n",
    "                ax6.set_xlabel('Error ($)', fontsize=12)\n",
    "                ax6.set_ylabel('Frequency', fontsize=12)\n",
    "                ax6.legend()\n",
    "                ax6.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{filename_prefix}_report.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.savefig(f\"{filename_prefix}_report.pdf\", bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"✅ Matplotlib report exported to {filename_prefix}_report.png and .pdf\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error exporting results: {e}\")\n",
    "\n",
    "# Export results\n",
    "export_results(metrics_to_use, results_to_use)\n",
    "\n",
    "# Create a summary report\n",
    "print(\"\\n📄 EXPORT SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(\"Files created:\")\n",
    "print(\"- gold_futures_forecast_metrics.csv: Model performance metrics\")\n",
    "print(\"- gold_futures_forecast_predictions.csv: All predictions and errors\")\n",
    "if plotly_available:\n",
    "    print(\"- gold_futures_forecast_dashboard.html: Interactive dashboard\")\n",
    "else:\n",
    "    print(\"- gold_futures_forecast_report.png/pdf: Static report\")\n",
    "\n",
    "print(\"\\n📁 All results have been exported to files.\")\n",
    "print(\"You can open the HTML dashboard in your web browser for interactive exploration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Interactive Model Comparison Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive model comparison widget\n",
    "def create_model_comparison_widget(results, metrics):\n",
    "    \"\"\"\n",
    "    Create interactive widget for model comparison\n",
    "    \"\"\"\n",
    "    model_names = list(results.keys())\n",
    "    \n",
    "    # Create widgets\n",
    "    model_selector = widgets.SelectMultiple(\n",
    "        options=model_names,\n",
    "        value=model_names[:2],  # Select first two models by default\n",
    "        description='Models:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    metric_selector = widgets.Dropdown(\n",
    "        options=['MASE', 'MAE', 'RMSE', 'MAPE', 'Directional_Accuracy'],\n",
    "        value='MASE',\n",
    "        description='Metric:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    def update_comparison(models_selected, metric_selected):\n",
    "        if len(models_selected) < 2:\n",
    "            print(\"Please select at least 2 models for comparison.\")\n",
    "            return\n",
    "        \n",
    "        # Create comparison chart\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add bars for selected models\n",
    "        values = [metrics[model][metric_selected] for model in models_selected]\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=list(models_selected),\n",
    "            y=values,\n",
    "            marker_color=['blue', 'red', 'green', 'orange', 'purple'][:len(models_selected)],\n",
    "            text=[f'{val:.4f}' for val in values],\n",
    "            textposition='auto'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Model Comparison - {metric_selected}',\n",
    "            xaxis_title='Model',\n",
    "            yaxis_title=metric_selected,\n",
    "            height=400\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # Print detailed comparison\n",
    "        print(f\"\\n{metric_selected} Comparison:\")\n",
    "        print(\"-\" * 40)\n",
    "        for model in models_selected:\n",
    "            print(f\"{model:20s}: {metrics[model][metric_selected]:8.4f}\")\n",
    "        \n",
    "        # Find best model\n",
    "        if metric_selected in ['MASE', 'MAE', 'RMSE', 'MAPE']:\n",
    "            best_model = min(models_selected, key=lambda x: metrics[x][metric_selected])\n",
    "            print(f\"\\n🏆 Best model (lowest {metric_selected}): {best_model}\")\n",
    "        else:\n",
    "            best_model = max(models_selected, key=lambda x: metrics[x][metric_selected])\n",
    "            print(f\"\\n🏆 Best model (highest {metric_selected}): {best_model}\")\n",
    "    \n",
    "    # Create interactive widget\n",
    "    interactive_widget = widgets.interactive(\n",
    "        update_comparison,\n",
    "        models_selected=model_selector,\n",
    "        metric_selected=metric_selector\n",
    "    )\n",
    "    \n",
    "    return interactive_widget\n",
    "\n",
    "# Create and display the interactive comparison tool\n",
    "comparison_widget = create_model_comparison_widget(results_to_use, metrics_to_use)\n",
    "display(comparison_widget)\n",
    "\n",
    "print(\"\\n🎛️ Interactive Model Comparison Tool:\")\n",
    "print(\"Select different models and metrics to compare performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Conclusions and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "1. **Model Performance**: This analysis evaluated the Chronos-Bolt-Base model against traditional baseline methods for gold futures forecasting using a rolling 3-month window approach.\n",
    "\n",
    "2. **Evaluation Framework**: The FEV-inspired evaluation framework provided standardized metrics (MASE, MAE, RMSE, MAPE, Directional Accuracy) for fair comparison across different forecasting approaches.\n",
    "\n",
    "3. **Interactive Analysis**: The interactive visualizations with zoom capabilities allow for detailed exploration of model performance across different time periods and market conditions.\n",
    "\n",
    "### Methodology Strengths\n",
    "\n",
    "- **Rolling Window**: 63-day (3 months) context window provides sufficient historical information while maintaining temporal relevance\n",
    "- **Comprehensive Metrics**: Multiple evaluation metrics capture different aspects of forecast quality\n",
    "- **Statistical Testing**: Diebold-Mariano tests provide statistical significance assessment\n",
    "- **Interactive Visualization**: Zoom-capable plots enable detailed investigation of model behavior\n",
    "\n",
    "### Limitations and Future Work\n",
    "\n",
    "1. **Extended Evaluation**: Test on additional time periods (2022-2024) for robustness\n",
    "2. **Multi-horizon Forecasting**: Evaluate performance for longer prediction horizons\n",
    "3. **Ensemble Methods**: Combine Chronos with traditional methods for improved performance\n",
    "4. **Market Regime Analysis**: Assess performance across different market conditions (bull/bear markets, high/low volatility)\n",
    "5. **Transaction Cost Analysis**: Include realistic trading costs in performance evaluation\n",
    "6. **Feature Engineering**: Explore additional features (technical indicators, sentiment data)\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "1. **Model Selection**: Choose models based on specific use case requirements (accuracy vs. interpretability)\n",
    "2. **Risk Management**: Implement proper risk controls when using forecasts for trading\n",
    "3. **Continuous Monitoring**: Regularly retrain and validate models on new data\n",
    "4. **Ensemble Approaches**: Consider combining multiple models for improved robustness\n",
    "\n",
    "### Technical Implementation\n",
    "\n",
    "The notebook demonstrates:\n",
    "- Proper data preprocessing for time series analysis\n",
    "- Integration of modern ML models (Chronos) with traditional baselines\n",
    "- Standardized evaluation using FEV principles\n",
    "- Interactive visualization for results exploration\n",
    "- Statistical significance testing for model comparison\n",
    "\n",
    "This framework can be extended to other financial time series and forecasting problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
